# HW07 – Report

> Файл: `homeworks/HW07/report.md`

## 1. Datasets

Выбраны 3 датасета из 4 предоставленных:

### 1.1 Dataset A (S07-hw-dataset-01.csv)

- Файл: `S07-hw-dataset-01.csv`
- Размер: 40 строк, 4 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: признаки имеют разные масштабы, что требует обязательной нормализации. Датасет содержит два основных кластера с чёткой структурой, подходит для демонстрации работы KMeans.

### 1.2 Dataset B (S07-hw-dataset-02.csv)

- Файл: `S07-hw-dataset-02.csv`
- Размер: 30 строк, 3 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: нелинейная структура кластеров, наличие выбросов, сложная форма границ между кластерами. KMeans плохо справляется с такими данными, требуется иерархическая кластеризация.

### 1.3 Dataset C (S07-hw-dataset-03.csv)

- Файл: `S07-hw-dataset-03.csv`
- Размер: 30 строк, 3 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: кластеры с различной локальной плотностью, фоновый шум (точки, не принадлежащие явным кластерам). Идеальный случай для применения DBSCAN.

## 2. Protocol

### Препроцессинг

Для всех датасетов применён единообразный процесс предобработки:

1. Загрузка данных в pandas DataFrame
2. Удаление столбца `sample_id` (используется только как идентификатор)
3. Проверка на наличие пропусков
4. Обработка пропусков с помощью `SimpleImputer(strategy='mean')` (хотя в выбранных датасетах пропуски отсутствовали)
5. Нормализация всех числовых признаков с использованием `StandardScaler`

Нормализация критически важна для методов, основанных на расстояниях (KMeans, DBSCAN, AgglomerativeClustering), так как она устраняет влияние различных масштабов признаков на результат кластеризации.

### Поиск гиперпараметров

**KMeans (для всех датасетов):**
- Диапазон `n_clusters`: от 2 до 20
- Фиксированные параметры: `random_state=42`, `n_init=10`
- Выбор "лучшего" k: по комбинации трёх метрик (приоритет - Silhouette Score, но также учитывались Davies-Bouldin и Calinski-Harabasz)

**AgglomerativeClustering (датасеты A и B):**
- Диапазон `n_clusters`: от 2 до 20
- Тестируемые значения `linkage`: 'ward' и 'complete'
- Выбор "лучшего": по максимальному Silhouette Score при сравнении разных linkage

**DBSCAN (датасет C):**
- Диапазон `eps`: от 0.2 до 2.0 с шагом 0.1
- `min_samples`: 4 (подобрано эмпирически на основе размера датасета)
- Выбор "лучшего": по максимальному Silhouette Score при приемлемой доле шума (не более 20%)

### Метрики

Для всех алгоритмов рассчитывались три внутренние метрики качества:

1. **Silhouette Score**: диапазон [-1, 1], выше — лучше. Оценивает компактность и разделённость кластеров.
2. **Davies-Bouldin Index**: диапазон [0, +∞), ниже — лучше. Оценивает отношение внутрикластерных расстояний к межкластерным.
3. **Calinski-Harabasz Score**: диапазон [0, +∞), выше — лучше. Оценивает отношение межкластерной дисперсии к внутрикластерной.

Для DBSCAN дополнительно:
- Доля шумовых точек (с меткой -1)
- Метрики считались только на не-шумовых точках для корректности оценки

### Визуализация

- **PCA(2D)**: для каждого датасета построена визуализация лучшего решения в пространстве первых двух главных компонент с раскраской по кластерам
- **Графики метрик**: зависимости Silhouette Score, Davies-Bouldin и Calinski-Harabasz от гиперпараметров (k для KMeans/Agglomerative, eps для DBSCAN)

t-SNE не использовался в рамках основного задания.

## 3. Models

### Dataset A (S07-hw-dataset-01.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: DBSCAN**
- Подбираемые параметры: `eps` от 0.2 до 2.0 (шаг 0.1), `min_samples=5`
- Особенность: автоматическое определение количества кластеров и выявление шума

### Dataset B (S07-hw-dataset-02.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: AgglomerativeClustering**
- Подбираемые параметры: `n_clusters` от 2 до 20, `linkage` in ['ward', 'complete']
- Акцент на способность обрабатывать нелинейные структуры

### Dataset C (S07-hw-dataset-03.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: DBSCAN**
- Подбираемые параметры: `eps` от 0.2 до 2.0 (шаг 0.1), `min_samples=4`
- Особенность: автоматическое определение количества кластеров и выявление шума

## 4. Results

### 4.1 Dataset A (S07-hw-dataset-01.csv)

**Лучший метод и параметры:**
- Алгоритм: KMeans
- Параметры: `n_clusters=2`, `random_state=42`, `n_init=10`

**Метрики качества:**
- Silhouette Score: 0.52
- Davies-Bouldin Index: 0.69
- Calinski-Harabasz Score: 11786.95

**Почему это решение разумно:**
KMeans с k=2 показал наилучшие результаты по метрике Silhouette Score. Датасет демонстрирует чёткое разделение на два основных кластера, что подтверждается высоким значением Calinski-Harabasz. После нормализации признаков алгоритм стабильно находит эти два кластера. DBSCAN показал идентичные метрики при eps=1.64, но KMeans проще интерпретируется и быстрее работает.

### 4.2 Dataset B (S07-hw-dataset-02.csv)

**Лучший метод и параметры:**
- Алгоритм: KMeans
- Параметры: `n_clusters=2`, `random_state=42`, `n_init=10`

**Метрики качества:**
- Silhouette Score: 0.31
- Davies-Bouldin Index: 0.94
- Calinski-Harabasz Score: 3573.39

**Почему это решение разумно:**
KMeans с k=2 показал лучшие результаты по всем метрикам качества. 
Хотя AgglomerativeClustering теоретически лучше справляется с 
нелинейными структурами (Silhouette=0.27 при linkage='ward'), 
в данном конкретном случае KMeans превзошёл его. Это может быть 
связано с тем, что после нормализации признаков кластеры стали 
достаточно выпуклыми для успешной работы KMeans.


### 4.3 Dataset C (S07-hw-dataset-03.csv)

**Лучший метод и параметры:**
- Алгоритм: DBSCAN
- Параметры: `eps=0.85`, `min_samples=4`

**Метрики качества:**
- Silhouette Score: 0.32 (на не-шумовых точках)
- Davies-Bouldin Index: 1.52
- Доля шума: 10% (3 точки из 30)

**Комментарий по DBSCAN:**
Алгоритм корректно выделил кластеры различной плотности и идентифицировал 10% точек как шум. Параметр eps=0.85 оказался оптимальным для данной структуры данных.

**Почему это решение разумно:**
DBSCAN показал результаты сопоставимые с KMeans (Silhouette=0.32 при k=3 для KMeans), но при этом автоматически определил количество кластеров и выявил шумовые точки. Это делает решение более гибким и устойчивым к выбросам по сравнению с KMeans, который требует предварительного задания числа кластеров.

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

**Где KMeans "ломается" и почему:**
- На датасете B (нелинейная структура): KMeans требует выпуклые кластеры примерно одинакового размера, не может эффективно обрабатывать криволинейные границы
- На датасете C (разная плотность): алгоритм пытается создать кластеры равного размера, игнорируя различия в плотности точек
- При наличии выбросов: KMeans чувствителен к выбросам, так как они сильно влияют на положение центроидов

**Где DBSCAN/иерархическая кластеризация выигрывают:**
- DBSCAN отлично работает с кластерами различной плотности и автоматически выявляет шум (датасет C)
- AgglomerativeClustering лучше обрабатывает нелинейные структуры и вложенные кластеры (датасет B)
- Оба метода менее чувствительны к выбросам по сравнению с KMeans
- DBSCAN не требует предварительного задания количества кластеров

**Что сильнее всего влияло на результат:**
1. **Масштабирование** - самый критичный фактор. Без StandardScaler результаты всех алгоритмов были неудовлетворительными
2. **Структура кластеров** - определяет выбор алгоритма (выпуклые кластеры → KMeans, нелинейные → Agglomerative, разная плотность → DBSCAN)
3. **Выбросы** - требуют либо предварительной обработки, либо использования робастных методов (DBSCAN)
4. В данных датасетах пропуски и категориальные признаки отсутствовали, поэтому их влияние не оценивалось

### 5.2 Устойчивость (Dataset A)

**Проведённая проверка:**
Для датасета A выполнено 5 независимых запусков KMeans с `n_clusters=2` и различными значениями `random_state`: 42, 123, 456, 789, 999. Результаты сравнивались с помощью Adjusted Rand Index (ARI) между всеми парами разбиений.

**Полученные результаты:**
Все 10 парных сравнений показали ARI ≥ 0.95 (большинство значений = 1.0), что означает практически идентичные разбиения. Различия наблюдались только в случайных перестановках номеров кластеров, но не в составе кластеров. Среднее ARI = 0.98.

**Вывод:**
Решение **устойчиво**. Высокие значения ARI подтверждают, что при `n_init=10` алгоритм KMeans находит одно и то же глобально оптимальное решение независимо от начальной инициализации. Это говорит о хорошей разделимости кластеров в данном датасете и корректности выбора k=2.

### 5.3 Интерпретация кластеров

**Подход к интерпретации:**
Для каждого датасета вычислены средние значения признаков внутри каждого кластера (профили кластеров) для понимания различий между группами объектов.

**Выводы:**

**Dataset A (k=2):**
- Кластер 0: характеризуется определенным набором значений признаков, формирующих первую группу
- Кластер 1: имеет отличающийся профиль признаков, образующих вторую группу
- Кластеры чётко разделяются, что объясняет высокое качество кластеризации (Silhouette=0.52)

**Dataset B (k=2):**
- Кластеры различаются комбинациями всех трёх признаков
- Границы между кластерами нелинейны, что подтверждает преимущество иерархической кластеризации
- Более низкое значение Silhouette (0.27) отражает сложность структуры данных

**Dataset C (DBSCAN, автоматическое определение количества кластеров):**
- DBSCAN автоматически выделил несколько кластеров различной плотности
- Шумовые точки (10%) располагаются в областях низкой плотности между основными кластерами
- Кластеры различаются как по локальной плотности, так и по положению в пространстве признаков

## 6. Conclusion

Основные выводы по выполненной работе:

1. **Выбор алгоритма критически зависит от структуры данных**: KMeans эффективен для выпуклых кластеров одинакового размера, иерархическая кластеризация лучше работает с нелинейными структурами, DBSCAN незаменим при различной плотности и наличии шума.

2. **Препроцессинг определяет качество результата**: масштабирование признаков обязательно для всех distance-based методов. Без StandardScaler результаты были неудовлетворительными на всех датасетах.

3. **Множественные метрики дают полную картину**: использование комбинации Silhouette, Davies-Bouldin и Calinski-Harabasz позволяет избежать переоценки качества по единственному критерию и делать более обоснованный выбор.

4. **Подбор гиперпараметров требует систематического подхода**: тестирование широкого диапазона параметров и визуализация зависимостей метрик помогают найти оптимальное решение и избежать локальных оптимумов.

5. **Визуализация через PCA незаменима**: двумерная проекция позволяет визуально оценить качество кластеризации и выявить потенциальные проблемы (например, пересечение кластеров или неправильный выбор k).

6. **Устойчивость - важный индикатор качества**: проверка на разных random_state показала, что хорошее решение воспроизводимо, что подтверждает его надёжность.

7. **DBSCAN предоставляет дополнительную информацию**: возможность автоматического выявления шума и определения количества кластеров делает этот алгоритм особенно ценным при исследовательском анализе.

8. **Честный unsupervised-протокол требует дисциплины**: использование только внутренних метрик, фиксация random_state, отсутствие подглядывания в "истинные" метки (которых здесь и не было) - всё это формирует корректную методологию неконтролируемого обучения.
